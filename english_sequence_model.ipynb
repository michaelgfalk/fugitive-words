{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english_sequence_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelgfalk/fugitive-words/blob/master/english_sequence_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up5o6_-Inva1"
      },
      "source": [
        "# English Language Model for Foreign Word Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxVeDcGksyU5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, LSTM, CuDNNLSTM, Dropout, TimeDistributed, Masking\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # For one-hot encoding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive # For saving\n",
        "import pickle as p\n",
        "import regex as re\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Ny4zgYHTLI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92a038ba-9238-4a33-f3fd-011f49d1e5b5"
      },
      "source": [
        "# Link to Google drive for disk access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjY9UAbzy0qP"
      },
      "source": [
        "# set random seed for notebook\n",
        "random_seed = 425"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSEiD0mz8I9U"
      },
      "source": [
        "## 1. Import and clean training data\n",
        "\n",
        "For this task, we want a list of English words. The model will examine them to learn how letters follow one another in English.\n",
        "\n",
        "Training data has been harvested from the three large English corpora included in the Natural Language Toolkit: the 'brown' corpus of contemporary American English, the 'reuters' corpus of recent news articles, and the 'gutenberg' corpus, which comprises 18 literary texts, mostly from the Romantic period, but including a few plays of Shakespeare and the King James Bible. It has also been sourced from the 'lexicon' files for Contemporary and Historical American corpora on the BYU Corpus site. These corpora have between them ~5-10 million tokens, which equates to about 150,000 unique types in practice.\n",
        "\n",
        "Since no Australian or NZ corpus has been used, hopefully there are very few Papuan, Aboriginal and Austronesian words in the training set, and the model should learn to give a low probability to strings from those language families.\n",
        "\n",
        "The words will be exploded into characters, special characters and punctuation will be removed.\n",
        "\n",
        "**NB:** In earlier versions of this notebook I neglected to save the preprocessed data & the dict that defines how the tokenizer converts strings into a numeric representation. Don't forget it this time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td2wqkpNntO2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "eb6b2dc8-f5ee-41d9-e42c-152754041ae4"
      },
      "source": [
        "# Get training data from NLTK\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Vc6udBdv2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6e159b2-d8fe-45d6-b21b-cee7f2223b54"
      },
      "source": [
        "# Get training data from BYU corpora samples\n",
        "byu_words = []\n",
        "with open('/content/gdrive/My Drive/waves_of_words/byu_lexicons.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    line = line.rstrip() # remove trailing whitespace\n",
        "    byu_words.append(line) \n",
        "\n",
        "# Sanity check:\n",
        "print(f'There are {len(byu_words)} unique types in the byu corpora. The first five are:\\n {byu_words[0:4]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 114097 unique types in the byu corpora. The first five are:\n",
            " ['word', 'the', 'and', 'of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr4GcIhr4z0Y"
      },
      "source": [
        "# Combine into single wordlist\n",
        "words = set(nltk.corpus.brown.words() + nltk.corpus.reuters.words() + nltk.corpus.gutenberg.words() + byu_words)\n",
        "words = list(words)\n",
        "\n",
        "# Regex for stripping special characters\n",
        "regex = re.compile(r'[\\W\\d_]') # Match any non-word character, digit or underscore\n",
        "\n",
        "# Clean\n",
        "\n",
        "# Set all words to lower case, and add start and end characters:\n",
        "words = set(['S' + word.lower() + 'E' for word in words])\n",
        "# Get rid of digits and non-word characters:\n",
        "words = set([regex.sub('', word) for word in words])\n",
        "# Convert set to list:\n",
        "words = list(words)\n",
        "# Get rid of empty strings/junk\n",
        "words = [word for word in words if len(word) > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKJ5r2ZEfXbV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0dce6be8-3f67-4a06-b8c5-842b5fab8d3e"
      },
      "source": [
        "# Have another look at the data\n",
        "print(f'There are {len(words)} unique types in the training data.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 149849 unique types in the training data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWG_l7xQxUYA"
      },
      "source": [
        "# Format for tensorflow\n",
        "tkzr = Tokenizer(lower = False, char_level = True, oov_token = \"?\", filters = None) # out-of-vocab character represented by ?\n",
        "tkzr.fit_on_texts(words)\n",
        "seq_list = tkzr.texts_to_sequences(words)\n",
        "data = pad_sequences(seq_list, padding = 'pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDj0n8CpfhtY"
      },
      "source": [
        "char_to_int = tkzr.word_index\n",
        "int_to_char = {value:key for key,value in tkzr.word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahCaXV7LSu2q"
      },
      "source": [
        "# Save training data and word_index to Google Drive\n",
        "with open('/content/gdrive/My Drive/waves_of_words/20190215_seq_data.p', 'wb') as file:\n",
        "  p.dump({\"data\":data,\"tkzr\":tkzr}, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpyb4AYfVtnC"
      },
      "source": [
        "with open('/content/gdrive/My Drive/waves_of_words/20190215_seq_data.p', 'rb') as file:\n",
        "  saved = p.load(file)\n",
        "\n",
        "data = saved['data']\n",
        "tkzr = saved['tkzr']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uZuCJoiqp9o"
      },
      "source": [
        "In the previous iteration of this notebook, I set up the data wrong. In that version, I just got the model to predict the last letter in the sequence.\n",
        "\n",
        "This model will work differently. Instead of predicting the last character, it will predict the next character. This means that $X$ and $Y$ will ahve the same shape: $(m, t-1, n)$, where $m$ is the number of training examples, $t-1$ the maximum length of the sequences minus 1, and $n$ the number of features (in this case, characters in the alphabet, plus $0$ for no character, $1$ for an unknown character, and the two special characters $S$ and $E$, which represent the start and end of the word respectively).\n",
        "\n",
        "$X$ will contain the characters from $0:t-1$, while $Y$ will contain the characters for $1:t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8bv6ScN_cVz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d0e7da75-d53c-488e-f737-60cd65cca424"
      },
      "source": [
        "# Get dimensions\n",
        "# Convert data to one-hot encoding\n",
        "print(f'Data dimensions before one-hot-encoding: {data.shape}')\n",
        "data = tf.keras.utils.to_categorical(data, dtype = 'float32') # For some reason Tensorflow requires a float input\n",
        "print(f'Data dimensions after one-hot-encoding: {data.shape}')\n",
        "data = data[:,:,1:] # Drop padding (we don't want the LSTM to learn a feature for an empty timestep)\n",
        "print(f'Data dimensions after dropping padding feature: {data.shape}')\n",
        "m, t, n = data.shape\n",
        "X = data[:,0:-1,:]\n",
        "Y = data[:,1:,:]\n",
        "\n",
        "# Shuffle and split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = random_seed)\n",
        "\n",
        "# Sanity check\n",
        "print(f'The shape of X_train is {X_train.shape}')\n",
        "print(f'The shape of X_test is {X_test.shape}')\n",
        "print(f'The shape of Y_train is {Y_train.shape}')\n",
        "print(f'The shape of Y_test is {Y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data dimensions before one-hot-encoding: (149849, 30)\n",
            "Data dimensions after one-hot-encoding: (149849, 30, 35)\n",
            "Data dimensions after dropping padding feature: (149849, 30, 34)\n",
            "The shape of X_train is (134864, 29, 34)\n",
            "The shape of X_test is (14985, 29, 34)\n",
            "The shape of Y_train is (134864, 29, 34)\n",
            "The shape of Y_test is (14985, 29, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lK4gsIu8awu"
      },
      "source": [
        "## 2. Instantiate and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWE-bvtH8aSK"
      },
      "source": [
        "def init_lstm(max_time, num_features, hidden_state_dim = 10, rnn_layers = 3, drop_rate = 0.5):\n",
        "  \"\"\"\n",
        "  Implementation of a deep LSTM for sequence learning.\n",
        "  \n",
        "  params:\n",
        "    max_time (int): the maximum sequence length in the training data\n",
        "    num_features (int): the number of individual characters in the training set\n",
        "    lstm_hidden (int): the size of the hidden state in the LSTM cells\n",
        "    num_rnn_layers (int): the number of LSTM layers desired\n",
        "    drop_rate (float: 0 < x =< 1): the number of inputs to randomly ignore in the Dropout layers\n",
        "  \n",
        "  returns:\n",
        "    lstm_net: a Keras Model() object\n",
        "  \"\"\"\n",
        "  \n",
        "  # Define input to model\n",
        "  # NB: Althought an integer input might seem to make sense for one-hot encoding, tf requires a float input\n",
        "  seq_in = Input(shape = (max_time, num_features), dtype = 'float32', name = \"seq_in\")\n",
        "  \n",
        "  # Hidden layers\n",
        "  for i in range(rnn_layers):\n",
        "    if i == 0:\n",
        "      X = CuDNNLSTM(units = hidden_state_dim, return_sequences = True, name = \"lstm_\" + str(i))(seq_in)\n",
        "    else:\n",
        "      X = CuDNNLSTM(units = hidden_state_dim, return_sequences = True, name = \"lstm_\" + str(i))(X)\n",
        "    X = Dropout(rate = drop_rate, name = \"dropout_\" + str(i))(X)\n",
        "  \n",
        "  # Activation layer (apply to each timestep)\n",
        "  Yhat = TimeDistributed(Dense(num_features, activation = \"softmax\"))(X)\n",
        "  \n",
        "  # Create model\n",
        "  lstm_net = Model(inputs = [seq_in], outputs = Yhat)\n",
        "  \n",
        "  return lstm_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZdm_SzVMEEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "ab8a7049-1ae0-4962-82ee-4d1b4dcf3ba1"
      },
      "source": [
        "# Set hyperparameters\n",
        "hd = 150\n",
        "l = 4\n",
        "dr = 0.3\n",
        "\n",
        "# Initialise model\n",
        "model = init_lstm(max_time = t-1, num_features = n, hidden_state_dim = hd, rnn_layers = l, drop_rate = dr)\n",
        "\n",
        "# Sanity check\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "seq_in (InputLayer)          (None, 29, 34)            0         \n",
            "_________________________________________________________________\n",
            "lstm_0 (CuDNNLSTM)           (None, 29, 150)           111600    \n",
            "_________________________________________________________________\n",
            "dropout_0 (Dropout)          (None, 29, 150)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (CuDNNLSTM)           (None, 29, 150)           181200    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 29, 150)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (CuDNNLSTM)           (None, 29, 150)           181200    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 29, 150)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (CuDNNLSTM)           (None, 29, 150)           181200    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 29, 150)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 29, 34)            5134      \n",
            "=================================================================\n",
            "Total params: 660,334\n",
            "Trainable params: 660,334\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QssTLmt-Ej4k"
      },
      "source": [
        "# Create optimizer and compile\n",
        "opt = tf.keras.optimizers.Adam(clipnorm = 5)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxpt8BUVHxps",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "e5de57da-1e0e-4238-bf92-8a3ab4e82795"
      },
      "source": [
        "# Train new model using class weights and save\n",
        "model.fit(x = X_train, y = Y_train,\n",
        "                      batch_size = 128, epochs = 20,\n",
        "                      validation_data = [X_test, Y_test],\n",
        "                      verbose = 1)\n",
        "\n",
        "\n",
        "model.save('/content/gdrive/My Drive/waves_of_words/20190220_model_more_layers_20_epochs.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 134864 samples, validate on 14985 samples\n",
            "Epoch 1/20\n",
            "134864/134864 [==============================] - 74s 548us/sample - loss: 0.5909 - val_loss: 0.5699\n",
            "Epoch 2/20\n",
            "134864/134864 [==============================] - 55s 406us/sample - loss: 0.5889 - val_loss: 0.5693\n",
            "Epoch 3/20\n",
            "134864/134864 [==============================] - 55s 408us/sample - loss: 0.5880 - val_loss: 0.5682\n",
            "Epoch 4/20\n",
            "134864/134864 [==============================] - 55s 409us/sample - loss: 0.6157 - val_loss: 0.5736\n",
            "Epoch 5/20\n",
            "134864/134864 [==============================] - 55s 408us/sample - loss: 0.5928 - val_loss: 0.5694\n",
            "Epoch 6/20\n",
            "134864/134864 [==============================] - 55s 410us/sample - loss: 0.5892 - val_loss: 0.5682\n",
            "Epoch 7/20\n",
            "134864/134864 [==============================] - 55s 406us/sample - loss: 0.5875 - val_loss: 0.5680\n",
            "Epoch 8/20\n",
            "134864/134864 [==============================] - 55s 407us/sample - loss: 0.5861 - val_loss: 0.5665\n",
            "Epoch 9/20\n",
            "134864/134864 [==============================] - 55s 405us/sample - loss: 0.5985 - val_loss: 0.5693\n",
            "Epoch 10/20\n",
            "134864/134864 [==============================] - 55s 406us/sample - loss: 0.5871 - val_loss: 0.5667\n",
            "Epoch 11/20\n",
            "134864/134864 [==============================] - 55s 405us/sample - loss: 0.5851 - val_loss: 0.5654\n",
            "Epoch 12/20\n",
            "134864/134864 [==============================] - 55s 405us/sample - loss: 0.5840 - val_loss: 0.5645\n",
            "Epoch 13/20\n",
            "134864/134864 [==============================] - 54s 402us/sample - loss: 0.5828 - val_loss: 0.5643\n",
            "Epoch 14/20\n",
            "134864/134864 [==============================] - 55s 406us/sample - loss: 0.5819 - val_loss: 0.5632\n",
            "Epoch 15/20\n",
            "134864/134864 [==============================] - 55s 404us/sample - loss: 0.5811 - val_loss: 0.5625\n",
            "Epoch 16/20\n",
            "134864/134864 [==============================] - 55s 405us/sample - loss: 0.5849 - val_loss: 0.5621\n",
            "Epoch 17/20\n",
            "134864/134864 [==============================] - 55s 405us/sample - loss: 0.5793 - val_loss: 0.5615\n",
            "Epoch 18/20\n",
            "134864/134864 [==============================] - 55s 404us/sample - loss: 0.5787 - val_loss: 0.5611\n",
            "Epoch 19/20\n",
            "134864/134864 [==============================] - 54s 402us/sample - loss: 0.5784 - val_loss: 0.5611\n",
            "Epoch 20/20\n",
            "134864/134864 [==============================] - 54s 402us/sample - loss: 0.5779 - val_loss: 0.5604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmQo49k39aRs"
      },
      "source": [
        "## 3. Evaluate the model\n",
        "\n",
        "Now the test. Does the model assign a higher or lower probability to Aboriginal words than to unseen English words?\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "*20 Feb 2019:* It doesn't seem to make much difference if you train a much deeper model. The model seemed to make just the same kind of prediciton, even when I increased the number of hidden layers from 2 to 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iOcf4-tJAKa"
      },
      "source": [
        "# Set paths for saved model and training/test data\n",
        "model_path = '/content/gdrive/My Drive/waves_of_words/20190220_model_more_layers_20_epochs.h5'\n",
        "data_path = '/content/gdrive/My Drive/waves_of_words/20190215_seq_data.p'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLDFViv2-GRM"
      },
      "source": [
        "# RUN THIS CELL IF YOU NEED TO EVALUATE A SAVED MODEL\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Load training and test data\n",
        "with open(data_path,'rb') as file:\n",
        "  save_dict = p.load(file)\n",
        "\n",
        "# Unpack data\n",
        "data = save_dict['data']\n",
        "tkzr = save_dict['tkzr']\n",
        "\n",
        "# Reshape data\n",
        "\n",
        "data = tf.keras.utils.to_categorical(data, dtype = 'float32') # For some reason Tensorflow requires a float input\n",
        "data = data[:,:,1:] # Drop padding (we don't want the LSTM to learn a feature for an empty timestep)\n",
        "m, t, n = data.shape\n",
        "X = data[:,0:-1,:]\n",
        "Y = data[:,1:,:]\n",
        "\n",
        "# Shuffle and split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em0Ulg1lQ_sY"
      },
      "source": [
        "The following helper functions can be used to retrieve sequence probabilities, and also to prepare texts for processing by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRNIQXYxw4hd"
      },
      "source": [
        "def get_seq_probs(data, model, padding_removed = True):\n",
        "  \"\"\"\n",
        "  Given data, get the model's predicted probabilties for each sequence.\n",
        "  \n",
        "  Args:\n",
        "    data (np.array): a 3d-tensor of dimensions examples x time-steps x num_features\n",
        "    model (keras.Model): a trained RNN\n",
        "    \n",
        "  Returns:\n",
        "    probs (np.array): a 1d numpy array of the probability of each sequence\n",
        "  \"\"\"\n",
        "  \n",
        "  # Split data into X and Y\n",
        "  if padding_removed:\n",
        "    X = data[:,:-1,:]\n",
        "    Y = data[:,1:,:]\n",
        "  else:\n",
        "    X = data[:,:-1,1:]\n",
        "    Y = data[:,1:,1:]\n",
        "  \n",
        "  # Get predictions for X\n",
        "  pred = model.predict(X)\n",
        "  \n",
        "  # Compute product for each sequence\n",
        "  pred = np.ma.masked_array(pred, Y == 0) # Get probs for the correct letters\n",
        "  pred = np.sum(pred, axis = 2) # Sum over feature vectors\n",
        "  pred = np.prod(pred, axis = 1) # Multiply over timesteps\n",
        "  pred = np.ma.compressed(pred)\n",
        "  \n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc5SJ_ONOC56"
      },
      "source": [
        "def process_texts(words, tkzr, maxlen = 30):\n",
        "  \"\"\"\n",
        "  Given a list of texts and a tokeniser, creates one-hot matrix.\n",
        "  \n",
        "  Aguments:\n",
        "    text_list (list): the texts (in this case, words)\n",
        "    tkzr (keras.preprocessing.Tokenizer): a Tokeniser that has been fit on the data\n",
        "    \n",
        "  Returns:\n",
        "    data (np.array): a 3d numpy array of dimensions m x t x n, with the padding\n",
        "                     category removed\n",
        "  \"\"\"\n",
        "  ## CLEAN UP TEXT\n",
        "  \n",
        "  # Regex for stripping special characters\n",
        "  regex = re.compile(r'[\\W\\d_]') # Match any non-word character, digit or underscore\n",
        "  # Make sure all the words are strings\n",
        "  words = [word for word in words if type(word) == str]\n",
        "  # Get rid of digits and non-word characters:\n",
        "  words = set([regex.sub('', word) for word in words])\n",
        "  # Set all words to lower case, and add start and end characters:\n",
        "  words = set(['S' + word.lower() + 'E' for word in words])\n",
        "  # Convert set to list:\n",
        "  words = list(words)\n",
        "  # Get rid of empty strings/junk\n",
        "  words = [word for word in words if len(word) > 0]\n",
        "  print(f'Text data cleaned: there are {len(words)} texts in the corpus.')\n",
        "  \n",
        "  ## CONVERT TO BINARY REPRESENTATION\n",
        "  # t and n are fixed by the model/tokenzier:\n",
        "  t = maxlen\n",
        "  n = len(tkzr.word_index) + 1\n",
        "  \n",
        "  seq_list = tkzr.texts_to_sequences(words) # Convert to list of feature vectors\n",
        "  data = pad_sequences(seq_list, padding = 'pre', truncating = 'post', maxlen = t) # Convert to matrix of fixed width\n",
        "  data = tf.keras.utils.to_categorical(data, dtype = 'float32', num_classes = n) # One-hot encode\n",
        "  data = data[:,:,1:] # Remove padding feature from 3-tensor\n",
        "  print(f'Data converted to binary representation. It has dimensions: {data.shape}')\n",
        "  \n",
        "  return(data)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVekHmMn1hWV"
      },
      "source": [
        "def reconstruct_sequences(data, tkzr, padding_removed = True):\n",
        "  \"\"\"\n",
        "  Reconstruct the sequences from the data matrix using the tokeniser.\n",
        "  \n",
        "  Arguments:\n",
        "    data (np.ndarray): a 3d array of one-hot encoded sequences\n",
        "    tkzr (keras_preprocessing.text.Tokenizer): the Tokeniser used to preprocess\n",
        "          the data\n",
        "  \n",
        "  Returns:\n",
        "    seqs (list): the words from the data\n",
        "  \"\"\"\n",
        "  \n",
        "  int_to_char = {value:key for key,value in tkzr.word_index.items()}\n",
        "  int_to_char[0] = \"\" # Add padding variable to index\n",
        "  \n",
        "  if padding_removed:\n",
        "    # Add padding feature back to data\n",
        "    m, t, n = data.shape\n",
        "    pad_slice = np.zeros(shape = (m, t, 1))\n",
        "    data = np.concatenate([pad_slice, data], axis = 2)\n",
        "  \n",
        "  # Convert binary matrix to dense\n",
        "  indices = np.argmax(data, axis = -1)\n",
        "  \n",
        "  # Convert indices to characters\n",
        "  seqs = np.apply_along_axis(lambda row : [int_to_char[x] for x in row], axis = 1, arr = indices)\n",
        "  # Concatenate characters\n",
        "  seqs = [''.join(row) for row in seqs]\n",
        "  \n",
        "  return(seqs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMTedSWBRMiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fa16a70e-39c4-4fe0-8058-318d9718db18"
      },
      "source": [
        "# Import Australian words, and see how the model does on them\n",
        "gamilaraay = pd.read_excel('/content/gdrive/My Drive/waves_of_words/GamilaraayExport.xlsx')\n",
        "gamilaraay = gamilaraay['OriginalForm'].tolist()\n",
        "# Remove everything after the comma\n",
        "gamilaraay = [word for word in gamilaraay if type(word) == str]\n",
        "comma = re.compile(r',.+')\n",
        "gamilaraay = [comma.sub('', word) for word in gamilaraay]\n",
        "print(gamilaraay[0:10])\n",
        "\n",
        "# Predict\n",
        "gam_data = process_texts(gamilaraay, tkzr)\n",
        "gam_probs = get_seq_probs(gam_data, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['girran', '-bidi', 'yii-li', 'buluuy', 'yilaalu', 'galiya-y', 'yu-gi', 'garra-li', 'buruma', 'ngadaa']\n",
            "Text data cleaned: there are 4879 texts in the corpus.\n",
            "Data converted to binary representation. It has dimensions: (4879, 30, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CemkzjeVTMsw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "814ce904-d537-4bc1-cb55-ce125543ec2e"
      },
      "source": [
        "# Try with another Australian language\n",
        "gunaikurnai = pd.read_excel('/content/gdrive/My Drive/waves_of_words/KurnaiExport.xlsx')\n",
        "gunaikurnai = gunaikurnai['OriginalForm'].tolist()\n",
        "# Remove everything after the comma\n",
        "gunaikurnai = [word for word in gunaikurnai if type(word) == str]\n",
        "comma = re.compile(r',.+')\n",
        "gunaikurnai = [comma.sub('', word) for word in gunaikurnai]\n",
        "print(gunaikurnai[0:10])\n",
        "\n",
        "# Predict\n",
        "kur_data = process_texts(gunaikurnai, tkzr)\n",
        "kur_probs = get_seq_probs(kur_data, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jirrah', 'wadhan', 'baan', 'ngooran', 'miowera', 'wrang', 'jellangoong', 'booran', 'wokook', 'kooragan']\n",
            "Text data cleaned: there are 3319 texts in the corpus.\n",
            "Data converted to binary representation. It has dimensions: (3319, 30, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTB0uyTVxadd"
      },
      "source": [
        "test_data = np.concatenate([X_test, Y_test[:,[-1],:]], axis = 1)\n",
        "eng_probs = get_seq_probs(test_data, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR0HjrfDSV_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "826bcae0-9fe5-4bc4-e564-fd2225249b27"
      },
      "source": [
        "print(f'The average probability of a Training word is {np.mean(eng_probs):.6f}')\n",
        "print(f'The average probability of a Gamilaraay word is {np.mean(gam_probs):.6f}')\n",
        "print(f'The average probability of a Gunaikurnai word is {np.mean(kur_probs):.6f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average probability of a Training word is 0.000089\n",
            "The average probability of a Gamilaraay word is 0.000257\n",
            "The average probability of a Gunaikurnai word is 0.000008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xlRzeYZUzHZ"
      },
      "source": [
        "*Sigh* Perhaps we can try normalising for length..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap0Jv6dcVGrI"
      },
      "source": [
        "gam_pred = model.predict(gam_data[:,:-1,:])\n",
        "eng_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6FcuwU5vxf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8b74667a-8c7a-4987-a2c2-2073638bd465"
      },
      "source": [
        "print(f'The mean probabilility in the prediction matrix for Gamilaraay is: {np.mean(gam_pred)}.')\n",
        "print(f'The mean probabilility in the prediction matrix for English is: {np.mean(eng_pred)}.')\n",
        "print(f'The mean probabilility in the masked matrix for Gamilaraay is: {np.mean(np.ma.masked_array(gam_pred, gam_data[:,1:,:] == 0))}.')\n",
        "print(f'The mean probabilility in the masked matrix for English is: {np.mean(np.ma.masked_array(eng_pred, Y_test == 0))}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean probabilility in the prediction matrix for Gamilaraay is: 0.029411764815449715.\n",
            "The mean probabilility in the prediction matrix for English is: 0.029411761090159416.\n",
            "The mean probabilility in the masked matrix for Gamilaraay is: 0.2770350196576344.\n",
            "The mean probabilility in the masked matrix for English is: 0.420858431607581.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLqLppspwp5K"
      },
      "source": [
        "# Let's see what effect the length of the sequence is having ...\n",
        "eng_len = np.sum(test_data, axis = 2) # Sum over third dimension - now each sequence looks like [0,0,0,0, ... 0,1,1,1 ... 1]\n",
        "eng_len = np.sum(eng_len, axis = 1) # Sum over second dimension to get lengths of each sequence\n",
        "\n",
        "gam_len = np.sum(gam_data, axis = 2)\n",
        "gam_len = np.sum(gam_len, axis = 1)\n",
        "\n",
        "kur_len = np.sum(kur_data, axis = 2)\n",
        "kur_len = np.sum(kur_len, axis = 1)\n",
        "\n",
        "# Reconstruct the words\n",
        "eng_words = reconstruct_sequences(test_data, tkzr)\n",
        "gam_words = reconstruct_sequences(gam_data, tkzr)\n",
        "kur_words = reconstruct_sequences(kur_data, tkzr)\n",
        "\n",
        "# Put into data frame\n",
        "eng_df = pd.DataFrame.from_dict({'eng_word':eng_words, 'seq_len':eng_len, 'eng_prob':eng_probs})\n",
        "gam_df = pd.DataFrame.from_dict({'gam_word':gam_words, 'seq_len':gam_len, 'gam_prob':gam_probs})\n",
        "kur_df = pd.DataFrame.from_dict({'kur_word':kur_words, 'seq_len':kur_len, 'kur_prob':kur_probs})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcDi6oIQz3Y7"
      },
      "source": [
        "eng_agg_probs = eng_df.groupby('seq_len').mean()\n",
        "gam_agg_probs = gam_df.groupby('seq_len').mean()\n",
        "kur_agg_probs = kur_df.groupby('seq_len').mean()\n",
        "\n",
        "eng_n = eng_df.groupby('seq_len').count().drop('eng_prob', 1)\n",
        "gam_n = gam_df.groupby('seq_len').count().drop('gam_prob', 1)\n",
        "kur_n = kur_df.groupby('seq_len').count().drop('kur_prob', 1)\n",
        "\n",
        "comb_probs = eng_agg_probs.join([eng_n, gam_agg_probs, gam_n, kur_agg_probs, kur_n])\n",
        "comb_probs['eng_gt_gam'] = comb_probs.eng_prob > comb_probs.gam_prob\n",
        "comb_probs['eng_gr_kur'] = comb_probs.eng_prob > comb_probs.kur_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soxE2QkfOSQh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "outputId": "8683f466-c289-4911-f586-378dea17171b"
      },
      "source": [
        "comb_probs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng_prob</th>\n",
              "      <th>eng_word</th>\n",
              "      <th>gam_prob</th>\n",
              "      <th>gam_word</th>\n",
              "      <th>kur_prob</th>\n",
              "      <th>kur_word</th>\n",
              "      <th>eng_gt_gam</th>\n",
              "      <th>eng_gr_kur</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seq_len</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>9.989445e-01</td>\n",
              "      <td>1</td>\n",
              "      <td>9.989445e-01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>4.217488e-02</td>\n",
              "      <td>2</td>\n",
              "      <td>4.099197e-02</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>1.842734e-03</td>\n",
              "      <td>58</td>\n",
              "      <td>1.880533e-03</td>\n",
              "      <td>32</td>\n",
              "      <td>2.264140e-03</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>1.264541e-04</td>\n",
              "      <td>401</td>\n",
              "      <td>1.628201e-04</td>\n",
              "      <td>111</td>\n",
              "      <td>1.771153e-04</td>\n",
              "      <td>71.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6.0</th>\n",
              "      <td>2.978990e-05</td>\n",
              "      <td>852</td>\n",
              "      <td>2.228185e-05</td>\n",
              "      <td>384</td>\n",
              "      <td>3.381302e-05</td>\n",
              "      <td>233.0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7.0</th>\n",
              "      <td>1.342543e-05</td>\n",
              "      <td>1296</td>\n",
              "      <td>3.452653e-06</td>\n",
              "      <td>757</td>\n",
              "      <td>5.293638e-06</td>\n",
              "      <td>404.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.0</th>\n",
              "      <td>7.389273e-06</td>\n",
              "      <td>1837</td>\n",
              "      <td>6.047894e-07</td>\n",
              "      <td>856</td>\n",
              "      <td>8.653188e-07</td>\n",
              "      <td>525.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.0</th>\n",
              "      <td>4.096907e-06</td>\n",
              "      <td>2066</td>\n",
              "      <td>5.155606e-08</td>\n",
              "      <td>787</td>\n",
              "      <td>1.652746e-07</td>\n",
              "      <td>488.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10.0</th>\n",
              "      <td>3.135183e-06</td>\n",
              "      <td>2024</td>\n",
              "      <td>5.867082e-09</td>\n",
              "      <td>571</td>\n",
              "      <td>2.359774e-08</td>\n",
              "      <td>395.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11.0</th>\n",
              "      <td>2.505302e-06</td>\n",
              "      <td>1749</td>\n",
              "      <td>3.456087e-10</td>\n",
              "      <td>368</td>\n",
              "      <td>1.632114e-10</td>\n",
              "      <td>274.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12.0</th>\n",
              "      <td>2.535585e-06</td>\n",
              "      <td>1437</td>\n",
              "      <td>7.515000e-11</td>\n",
              "      <td>265</td>\n",
              "      <td>2.797357e-11</td>\n",
              "      <td>239.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13.0</th>\n",
              "      <td>2.142210e-06</td>\n",
              "      <td>1073</td>\n",
              "      <td>5.050632e-13</td>\n",
              "      <td>170</td>\n",
              "      <td>2.288798e-12</td>\n",
              "      <td>138.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14.0</th>\n",
              "      <td>2.324961e-06</td>\n",
              "      <td>732</td>\n",
              "      <td>2.113646e-12</td>\n",
              "      <td>124</td>\n",
              "      <td>9.025425e-12</td>\n",
              "      <td>147.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15.0</th>\n",
              "      <td>2.554582e-06</td>\n",
              "      <td>541</td>\n",
              "      <td>3.210167e-15</td>\n",
              "      <td>73</td>\n",
              "      <td>3.683089e-13</td>\n",
              "      <td>77.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16.0</th>\n",
              "      <td>1.266478e-06</td>\n",
              "      <td>307</td>\n",
              "      <td>6.910515e-16</td>\n",
              "      <td>72</td>\n",
              "      <td>2.524150e-13</td>\n",
              "      <td>68.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17.0</th>\n",
              "      <td>3.245248e-06</td>\n",
              "      <td>265</td>\n",
              "      <td>2.352294e-18</td>\n",
              "      <td>36</td>\n",
              "      <td>1.331045e-15</td>\n",
              "      <td>45.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18.0</th>\n",
              "      <td>1.510789e-06</td>\n",
              "      <td>135</td>\n",
              "      <td>3.176841e-20</td>\n",
              "      <td>33</td>\n",
              "      <td>4.476576e-18</td>\n",
              "      <td>41.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19.0</th>\n",
              "      <td>9.816716e-07</td>\n",
              "      <td>80</td>\n",
              "      <td>6.186187e-22</td>\n",
              "      <td>36</td>\n",
              "      <td>3.594532e-18</td>\n",
              "      <td>33.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20.0</th>\n",
              "      <td>1.772319e-07</td>\n",
              "      <td>53</td>\n",
              "      <td>6.167094e-21</td>\n",
              "      <td>26</td>\n",
              "      <td>1.986495e-19</td>\n",
              "      <td>19.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21.0</th>\n",
              "      <td>1.500315e-07</td>\n",
              "      <td>37</td>\n",
              "      <td>1.465521e-23</td>\n",
              "      <td>15</td>\n",
              "      <td>4.423801e-21</td>\n",
              "      <td>26.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22.0</th>\n",
              "      <td>4.780537e-07</td>\n",
              "      <td>16</td>\n",
              "      <td>8.340297e-24</td>\n",
              "      <td>28</td>\n",
              "      <td>1.077701e-24</td>\n",
              "      <td>19.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23.0</th>\n",
              "      <td>2.929176e-11</td>\n",
              "      <td>14</td>\n",
              "      <td>1.955974e-25</td>\n",
              "      <td>16</td>\n",
              "      <td>5.388174e-26</td>\n",
              "      <td>13.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24.0</th>\n",
              "      <td>9.687662e-10</td>\n",
              "      <td>5</td>\n",
              "      <td>1.004625e-27</td>\n",
              "      <td>20</td>\n",
              "      <td>1.745667e-23</td>\n",
              "      <td>13.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25.0</th>\n",
              "      <td>3.121654e-10</td>\n",
              "      <td>2</td>\n",
              "      <td>1.614933e-25</td>\n",
              "      <td>6</td>\n",
              "      <td>8.407791e-45</td>\n",
              "      <td>3.0</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29.0</th>\n",
              "      <td>4.972087e-41</td>\n",
              "      <td>1</td>\n",
              "      <td>9.836893e-35</td>\n",
              "      <td>6</td>\n",
              "      <td>9.656590e-36</td>\n",
              "      <td>4.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30.0</th>\n",
              "      <td>1.013185e-37</td>\n",
              "      <td>1</td>\n",
              "      <td>3.575398e-38</td>\n",
              "      <td>60</td>\n",
              "      <td>1.094839e-30</td>\n",
              "      <td>18.0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             eng_prob  eng_word      gam_prob  gam_word      kur_prob  \\\n",
              "seq_len                                                                 \n",
              "2.0      9.989445e-01         1  9.989445e-01         1           NaN   \n",
              "3.0      4.217488e-02         2  4.099197e-02         4           NaN   \n",
              "4.0      1.842734e-03        58  1.880533e-03        32  2.264140e-03   \n",
              "5.0      1.264541e-04       401  1.628201e-04       111  1.771153e-04   \n",
              "6.0      2.978990e-05       852  2.228185e-05       384  3.381302e-05   \n",
              "7.0      1.342543e-05      1296  3.452653e-06       757  5.293638e-06   \n",
              "8.0      7.389273e-06      1837  6.047894e-07       856  8.653188e-07   \n",
              "9.0      4.096907e-06      2066  5.155606e-08       787  1.652746e-07   \n",
              "10.0     3.135183e-06      2024  5.867082e-09       571  2.359774e-08   \n",
              "11.0     2.505302e-06      1749  3.456087e-10       368  1.632114e-10   \n",
              "12.0     2.535585e-06      1437  7.515000e-11       265  2.797357e-11   \n",
              "13.0     2.142210e-06      1073  5.050632e-13       170  2.288798e-12   \n",
              "14.0     2.324961e-06       732  2.113646e-12       124  9.025425e-12   \n",
              "15.0     2.554582e-06       541  3.210167e-15        73  3.683089e-13   \n",
              "16.0     1.266478e-06       307  6.910515e-16        72  2.524150e-13   \n",
              "17.0     3.245248e-06       265  2.352294e-18        36  1.331045e-15   \n",
              "18.0     1.510789e-06       135  3.176841e-20        33  4.476576e-18   \n",
              "19.0     9.816716e-07        80  6.186187e-22        36  3.594532e-18   \n",
              "20.0     1.772319e-07        53  6.167094e-21        26  1.986495e-19   \n",
              "21.0     1.500315e-07        37  1.465521e-23        15  4.423801e-21   \n",
              "22.0     4.780537e-07        16  8.340297e-24        28  1.077701e-24   \n",
              "23.0     2.929176e-11        14  1.955974e-25        16  5.388174e-26   \n",
              "24.0     9.687662e-10         5  1.004625e-27        20  1.745667e-23   \n",
              "25.0     3.121654e-10         2  1.614933e-25         6  8.407791e-45   \n",
              "29.0     4.972087e-41         1  9.836893e-35         6  9.656590e-36   \n",
              "30.0     1.013185e-37         1  3.575398e-38        60  1.094839e-30   \n",
              "\n",
              "         kur_word  eng_gt_gam  eng_gr_kur  \n",
              "seq_len                                    \n",
              "2.0           NaN       False       False  \n",
              "3.0           NaN        True       False  \n",
              "4.0           2.0       False       False  \n",
              "5.0          71.0       False       False  \n",
              "6.0         233.0        True       False  \n",
              "7.0         404.0        True        True  \n",
              "8.0         525.0        True        True  \n",
              "9.0         488.0        True        True  \n",
              "10.0        395.0        True        True  \n",
              "11.0        274.0        True        True  \n",
              "12.0        239.0        True        True  \n",
              "13.0        138.0        True        True  \n",
              "14.0        147.0        True        True  \n",
              "15.0         77.0        True        True  \n",
              "16.0         68.0        True        True  \n",
              "17.0         45.0        True        True  \n",
              "18.0         41.0        True        True  \n",
              "19.0         33.0        True        True  \n",
              "20.0         19.0        True        True  \n",
              "21.0         26.0        True        True  \n",
              "22.0         19.0        True        True  \n",
              "23.0         13.0        True        True  \n",
              "24.0         13.0        True        True  \n",
              "25.0          3.0        True        True  \n",
              "29.0          4.0       False       False  \n",
              "30.0         18.0        True       False  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17JpXfFlScMW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d7480934-5e2c-4877-d1a2-cd48f9e0ddb5"
      },
      "source": [
        "# There is no linear correlation between the probability of the sequence and the length, but how about\n",
        "# a rank correlation?\n",
        "print(eng_df.corr(method = 'spearman'))\n",
        "print(gam_df.corr(method = 'spearman'))\n",
        "print(kur_df.corr(method = 'spearman'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          eng_prob  seq_len\n",
            "eng_prob   1.00000 -0.58779\n",
            "seq_len   -0.58779  1.00000\n",
            "          gam_prob   seq_len\n",
            "gam_prob  1.000000 -0.802803\n",
            "seq_len  -0.802803  1.000000\n",
            "          kur_prob   seq_len\n",
            "kur_prob  1.000000 -0.775826\n",
            "seq_len  -0.775826  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwlOE7gFV4W6"
      },
      "source": [
        "eng_stats = (eng_df.\n",
        "             groupby('seq_len').std().fillna(0).rename(columns = {'eng_prob':'eng_std'}).\n",
        "             join(\n",
        "                 eng_df.groupby('seq_len').mean().rename(columns = {'eng_prob':'eng_mean'})\n",
        "             ).\n",
        "             reset_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mchWJkAbWcl"
      },
      "source": [
        "# What if we try benchmarking using the probabilities of the training set?\n",
        "train_data = np.concatenate([X_train, Y_train[:,[-1],:]], axis = 1)\n",
        "train_probs = get_seq_probs(train_data, model)\n",
        "train_len = np.sum(train_data, axis = 2)\n",
        "train_len = np.sum(train_len, axis = 1)\n",
        "train_df = pd.DataFrame({'train_prob':train_probs, 'seq_len':train_len})\n",
        "eng_stats = (train_df.\n",
        "             groupby('seq_len').std().fillna(0).rename(columns = {'train_prob':'train_std'}).\n",
        "             join(\n",
        "                 train_df.groupby('seq_len').mean().rename(columns = {'train_prob':'train_mean'})\n",
        "             ).\n",
        "             reset_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDgC8hcmYT44"
      },
      "source": [
        "# Can we use the standard deviation to benchmark the Australian words?\n",
        "kur_df_merged = kur_df.merge(eng_stats, how = 'left', on = 'seq_len')\n",
        "gam_df_merged = gam_df.merge(eng_stats, how = 'left', on = 'seq_len')\n",
        "eng_df_merged = eng_df.merge(eng_stats, how = 'left', on = 'seq_len')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkE8VBcnZscb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "20771027-61e3-406b-c818-146822fcf8ea"
      },
      "source": [
        "# How many words are one standard deviation from the mean?\n",
        "s_factor = 0.2\n",
        "l_factor = 0\n",
        "r_factor = 30\n",
        "\n",
        "k = (kur_df_merged.train_mean - s_factor * kur_df_merged.train_std) > kur_df_merged.kur_prob\n",
        "g = (gam_df_merged.train_mean - s_factor * gam_df_merged.train_std) > gam_df_merged.gam_prob\n",
        "e = (eng_df_merged.train_mean - s_factor * eng_df_merged.train_std) > eng_df_merged.eng_prob\n",
        "\n",
        "k_f = (l_factor < kur_df_merged.seq_len) & (kur_df_merged.seq_len < r_factor)\n",
        "g_f = (l_factor < gam_df_merged.seq_len) & (gam_df_merged.seq_len < r_factor)\n",
        "e_f = (l_factor < eng_df_merged.seq_len) & (eng_df_merged.seq_len < r_factor)\n",
        "\n",
        "print(f'Only words with between {l_factor} and {r_factor} characters were considered.')\n",
        "print(f'{(k & k_f).sum()/k_f.sum():.2f} of the Gunaikurnai words are {s_factor} std from the mean of same-length English words')\n",
        "print(f'{(g & g_f).sum()/g_f.sum():.2f} of the Gamilaraay words have a probability {s_factor} std from the mean of same-length English words')\n",
        "print(f'{(e & e_f).sum()/e_f.sum():.2f} of the test English words have a probability {s_factor} std from the mean of same-length English words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Only words with between 0 and 30 characters were considered.\n",
            "0.83 of the Gunaikurnai words are 0.2 std from the mean of same-length English words\n",
            "0.87 of the Gamilaraay words have a probability 0.2 std from the mean of same-length English words\n",
            "0.64 of the test English words have a probability 0.2 std from the mean of same-length English words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROgXWwuZlTje"
      },
      "source": [
        "# What if we try comparing the means of the probabilities?\n",
        "gam_correct_probs = np.sum(np.ma.masked_array(gam_pred, gam_data[:,1:,:] == 0), axis = 2)\n",
        "gam_means = np.mean(gam_correct_probs, axis = 1)\n",
        "gam_mean_df = pd.DataFrame({'seq_len':gam_len, 'mean_prob':gam_means})\n",
        "\n",
        "eng_correct_probs = np.sum(np.ma.masked_array(eng_pred, Y_test == 0), axis = 2)\n",
        "eng_means = np.mean(eng_correct_probs, axis = 1)\n",
        "eng_mean_df = pd.DataFrame({'seq_len':eng_len, 'mean_prob':eng_means})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lrL9a8-Npq5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fef9ea49-bf58-4c37-9d9f-a427319e6baa"
      },
      "source": [
        "gm_quart = gam_mean_df.quantile([0.,0.25,0.5,0.75,1.])\n",
        "gm_quart"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_prob</th>\n",
              "      <th>seq_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.034828</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.25</th>\n",
              "      <td>0.259699</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.50</th>\n",
              "      <td>0.302355</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.75</th>\n",
              "      <td>0.342579</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.00</th>\n",
              "      <td>0.999472</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      mean_prob  seq_len\n",
              "0.00   0.034828      2.0\n",
              "0.25   0.259699      7.0\n",
              "0.50   0.302355      9.0\n",
              "0.75   0.342579     11.0\n",
              "1.00   0.999472     30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhvuHDgQOLrn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a49eb4aa-87bb-41e4-9f6d-f04e2ad0fd3f"
      },
      "source": [
        "em_quart = eng_mean_df.quantile([0.,0.25,0.5,0.75,1.])\n",
        "em_quart"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_prob</th>\n",
              "      <th>seq_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.133283</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.25</th>\n",
              "      <td>0.350209</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.50</th>\n",
              "      <td>0.398812</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.75</th>\n",
              "      <td>0.466562</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.00</th>\n",
              "      <td>0.999472</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      mean_prob  seq_len\n",
              "0.00   0.133283      2.0\n",
              "0.25   0.350209      8.0\n",
              "0.50   0.398812     10.0\n",
              "0.75   0.466562     12.0\n",
              "1.00   0.999472     30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Kw6edTOlSW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b4bc1779-3303-47bb-ba45-0e41c442dbb1"
      },
      "source": [
        "gp_quart = gam_df.quantile([0.,0.25,0.5,0.75,1.])\n",
        "gp_quart"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gam_prob</th>\n",
              "      <th>seq_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.25</th>\n",
              "      <td>2.060078e-14</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.50</th>\n",
              "      <td>6.177382e-10</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.75</th>\n",
              "      <td>2.199242e-07</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.00</th>\n",
              "      <td>9.989445e-01</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          gam_prob  seq_len\n",
              "0.00  0.000000e+00      2.0\n",
              "0.25  2.060078e-14      7.0\n",
              "0.50  6.177382e-10      9.0\n",
              "0.75  2.199242e-07     11.0\n",
              "1.00  9.989445e-01     30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cix5RgXAOpwR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3719d6c8-dcb3-4521-abbf-c21017da1051"
      },
      "source": [
        "ep_quart = eng_df.quantile([0.,0.25,0.5,0.75,1.])\n",
        "ep_quart"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng_prob</th>\n",
              "      <th>seq_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>4.972087e-41</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.25</th>\n",
              "      <td>8.643084e-09</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.50</th>\n",
              "      <td>2.846334e-07</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.75</th>\n",
              "      <td>3.944360e-06</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.00</th>\n",
              "      <td>9.989445e-01</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          eng_prob  seq_len\n",
              "0.00  4.972087e-41      2.0\n",
              "0.25  8.643084e-09      8.0\n",
              "0.50  2.846334e-07     10.0\n",
              "0.75  3.944360e-06     12.0\n",
              "1.00  9.989445e-01     30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8kZKmirO3hg"
      },
      "source": [
        "# What if the set the threshold at the third quartile for Gamilaraay?\n",
        "thresh = gm_quart.iloc[3,0]\n",
        "\n",
        "def predict(x):\n",
        "  if x < thresh:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92VGb-feTKKf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bc16cc3-cd26-4536-c819-4c420e350184"
      },
      "source": [
        "thresh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3425790203942193"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "193_gJyqPmb2"
      },
      "source": [
        "y = [1 for x in range(len(gam_means))] + [0 for x in range(len(eng_means))]\n",
        "y = np.array(y, dtype = 'int32')\n",
        "y_hat = [predict(x) for x in gam_means] + [predict(x) for x in eng_means]\n",
        "y_hat = np.array(y_hat, dtype = 'int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_38KxixQnBn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96d62766-1fbf-4cd9-a21c-4a96b53d594c"
      },
      "source": [
        "# Calculate precision and recall:\n",
        "true_positive = ((y == 1) & (y_hat == 1)).sum()\n",
        "false_positive = ((y == 0) & (y_hat == 1)).sum()\n",
        "false_negative = ((y == 1) & (y_hat == 0)).sum()\n",
        "\n",
        "precision = true_positive / (true_positive + false_positive)\n",
        "recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "print(f'When the threshold is set at {thresh:.8f}, the model can discriminate English from Gamilaraay with precision {precision:.2f} and recall {recall:.2f}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When the threshold is set at 0.34257902, the model can discriminate English from Gamilaraay with precision 0.53 and recall 0.75.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSE5vZ5ISN9-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}